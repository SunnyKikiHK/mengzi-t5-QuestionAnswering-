{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2ace02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'29-01-26-20_39_ckpt.pth'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "filename = datetime.now().strftime('%d-%m-%y-%H_%M_ckpt.pth')\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a6438c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "checkpoint = \"Langboat/mengzi-t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93774564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf004d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74975ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['阿', '斯顿', '撒', '啊', '阿', '德']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"阿斯顿撒啊阿德\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84d62a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[633, 20982, 3471, 196, 633, 273, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = tokenizer.encode(\"阿斯顿撒啊阿德\", max_length=10, padding=\"max_length\")\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b86cce25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'阿斯顿撒啊阿德'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97c3547b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'阿斯顿撒啊阿德</s><pad><pad><pad>'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "976742b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[633, 20982, 3471, 196, 633, 273, 1, 0, 0, 0], [2860, 180, 3100, 28, 1, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = tokenizer([\"阿斯顿撒啊阿德\", \"范文芳大\"], max_length=10, padding=\"max_length\", truncation=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb6294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import collote_fn\n",
    "\n",
    "model(**collote_fn(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333c5be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lamye\\miniconda3\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 5.94kB [00:00, 2.76MB/s]\n",
      "Downloading extra modules: 4.07kB [00:00, 2.92MB/s]                   \n",
      "Downloading extra modules: 3.34kB [00:00, 2.61MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvaluationModule(name: \"bleu\", module_type: \"metric\", features: [{'predictions': Value('string'), 'references': List(Value('string'))}, {'predictions': Value('string'), 'references': Value('string')}], usage: \"\"\"\n",
       "Computes BLEU score of translated segments against one or more references.\n",
       "Args:\n",
       "    predictions: list of translations to score.\n",
       "    references: list of lists of or just a list of references for each translation.\n",
       "    tokenizer : approach used for tokenizing `predictions` and `references`.\n",
       "        The default tokenizer is `tokenizer_13a`, a minimal tokenization approach that is equivalent to `mteval-v13a`, used by WMT.\n",
       "        This can be replaced by any function that takes a string as input and returns a list of tokens as output.\n",
       "    max_order: Maximum n-gram order to use when computing BLEU score.\n",
       "    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n",
       "Returns:\n",
       "    'bleu': bleu score,\n",
       "    'precisions': geometric mean of n-gram precisions,\n",
       "    'brevity_penalty': brevity penalty,\n",
       "    'length_ratio': ratio of lengths,\n",
       "    'translation_length': translation_length,\n",
       "    'reference_length': reference_length\n",
       "Examples:\n",
       "\n",
       "    >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "    >>> references = [\n",
       "    ...     [\"hello there general kenobi\", \"hello there!\"],\n",
       "    ...     [\"foo bar foobar\"]\n",
       "    ... ]\n",
       "    >>> bleu = evaluate.load(\"bleu\")\n",
       "    >>> results = bleu.compute(predictions=predictions, references=references)\n",
       "    >>> print(results[\"bleu\"])\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate \n",
    "\n",
    "evaluate.load('bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a905dad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [0.8571428571428571, 0.6, 0.3333333333333333, 0.0],\n",
       " 'brevity_penalty': 1.0,\n",
       " 'length_ratio': 1.1666666666666667,\n",
       " 'translation_length': 7,\n",
       " 'reference_length': 6}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "references = [\n",
    "         [\"hello there kenobi\", \"hello there!\"],\n",
    "         [\"foo bar foobar\"]\n",
    "     ]\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58058eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 32127, 21957, 32126,     1],\n",
      "        [    0, 32127,     1,     0,     0]])\n",
      "<pad> <extra_id_0> 解决这个问题 <extra_id_1> </s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'解决这个问题'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\n",
    "   [\"弄好\", \"解决\"],\n",
    "   padding=True,   \n",
    "   truncation=True, \n",
    "   return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = model.generate(input_ids)\n",
    "print(outputs)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
