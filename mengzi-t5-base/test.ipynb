{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b22ddf7",
   "metadata": {},
   "source": [
    "# Call library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c83cbeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate \n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from utils import read_json, merge_qa_dataset, collote_valid_fn, MAX_TARGET_LENGTH\n",
    "from dataset import MengziT5Dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm \n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "checkpoint = \"Langboat/mengzi-t5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291fa85b",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357d8f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 282/282 [00:00<00:00, 548.12it/s, Materializing param=shared.weight]                                                       \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "best_model_name = \"best_t5.pt\"\n",
    "foldername =  '31-01-26-15_14_ckpt'\n",
    "checkpoint_path = Path(f\"./checkpoint/{foldername}\")\n",
    "file_path = checkpoint_path / best_model_name\n",
    "\n",
    "checkpoint = \"Langboat/mengzi-t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "model.load_state_dict(torch.load(file_path, weights_only=True))\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80284dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSON file: 984it [00:00, 138250.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 984 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing to JSON file: 100%|██████████| 700/700 [00:00<00:00, 84983.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Merged data saved to data/formatted_dev.json\n",
      "Original count: 984 -> New count: 700\n",
      "Total data filtered away: 19\n",
      "test input_ids:  tensor([[  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0]])\n",
      "test attention_mask:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "test decoder_input_ids:  tensor([[   0,    7, 1000,  ...,    0,    0,    0],\n",
      "        [   0,    7,  935,  ...,    0,    0,    0],\n",
      "        [   0,    7,  743,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   0,    7,    8,  ...,    0,    0,    0],\n",
      "        [   0,    7, 7495,  ...,    0,    0,    0],\n",
      "        [   0,    7, 5625,  ...,    0,    0,    0]])\n",
      "test labels: tensor([[   7, 1000, 6434,  ..., -100, -100, -100],\n",
      "        [   7,  935, 1123,  ..., -100, -100, -100],\n",
      "        [   7,  743, 1656,  ..., -100, -100, -100],\n",
      "        ...,\n",
      "        [   7,    8, 1536,  ..., -100, -100, -100],\n",
      "        [   7, 7495,  236,  ..., -100, -100, -100],\n",
      "        [   7, 5625,   38,  ..., -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = \"data/train.json\"\n",
    "DATA_DEV_PATH = \"data/dev.json\"\n",
    "\n",
    "DATA_FDEV_PATH = \"data/formatted_dev.json\"\n",
    "DATA_DEV_PATH = \"data/dev.json\"\n",
    "\n",
    "test_batch_size = 8\n",
    "\n",
    "valid_data = read_json(DATA_DEV_PATH)\n",
    "merged_valid_data = merge_qa_dataset(valid_data, DATA_FDEV_PATH)\n",
    "valid_dataset = MengziT5Dataset(merged_valid_data, tokenizer)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "_, test_dataset = random_split(valid_dataset, [0.5, 0.5], generator=generator)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=test_batch_size, collate_fn=lambda x: collote_valid_fn(x, model, tokenizer))\n",
    "test_data = next(iter(test_dataloader))\n",
    "print(\"test input_ids: \", test_data['input_ids'])\n",
    "print(\"test attention_mask: \", test_data['attention_mask'])\n",
    "print(\"test decoder_input_ids: \", test_data['decoder_input_ids'])\n",
    "print(\"test labels:\", test_data['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19808c",
   "metadata": {},
   "source": [
    "# Model with 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea22b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, tokenizer):\n",
    "    model.eval()\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    loss = []\n",
    "    val_loss_sum = 0.0\n",
    "\n",
    "    #cumulative_batch = (epoch-1) * len(dataloader)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(dataloader, start=1):\n",
    "                raw_references = batch_data.pop(\"answer\", None)\n",
    "                if raw_references is None:\n",
    "                    print(\"No raw reference is found. Now create based on labels.\")\n",
    "                    temp_labels = torch.where(batch_data[\"labels\"] != -100, batch_data[\"labels\"], tokenizer.pad_token_id)\n",
    "                    raw_references = [[ref] for ref in tokenizer.batch_decode(temp_labels, skip_special_tokens=True)]\n",
    "\n",
    "\n",
    "                batch_data = batch_data.to(device)\n",
    "                results = model(**batch_data)\n",
    "                loss = results.loss\n",
    "                val_loss_sum += loss.item() # Accumulate loss\n",
    "\n",
    "                outputs = model.generate(\n",
    "                    batch_data[\"input_ids\"],\n",
    "                    attention_mask=batch_data[\"attention_mask\"],\n",
    "                    max_new_tokens=MAX_TARGET_LENGTH,\n",
    "                    num_beams=4\n",
    "                    )\n",
    "                decoded_outputs = tokenizer.batch_decode(\n",
    "                    outputs,\n",
    "                    skip_special_tokens=True\n",
    "                    )\n",
    "\n",
    "                batch_preds = []\n",
    "                for pred in decoded_outputs:\n",
    "                    if len(pred) == 0:\n",
    "                        pred = \" \" # Prevent divided by zero during calculation of BLEU\n",
    "                    pred = ' '.join(pred.strip()) # 'A B C' \n",
    "                    batch_preds.append(pred)\n",
    "                \n",
    "                batch_labels = []\n",
    "                for ref_list in raw_references: # ref_list: [ans1, ans2, ...]\n",
    "                    processed_ref_list = []\n",
    "                    for ref in ref_list:\n",
    "                        cleaned_ref = ref.strip()\n",
    "                        processed_ref_list.append(' '.join(cleaned_ref.strip()))\n",
    "                    batch_labels.append(processed_ref_list)\n",
    "\n",
    "\n",
    "                all_preds.extend(batch_preds)\n",
    "                all_labels.extend(batch_labels)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "            bleu_result = bleu.compute(predictions=all_preds, references=all_labels)\n",
    "            result = {f\"bleu-{i}\" : value for i, value in enumerate(bleu_result[\"precisions\"], start=1)}\n",
    "            result['avg'] = bleu_result['bleu']\n",
    "            avg_val_loss = val_loss_sum / len(dataloader)\n",
    "            log_dict = {\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"BLEU_avg\": bleu_result['bleu'], # 'bleu' is the avg in huggingface evaluate\n",
    "                \"BLEU_1\": bleu_result['precisions'][0],\n",
    "                \"BLEU_2\": bleu_result['precisions'][1],\n",
    "                \"BLEU_3\": bleu_result['precisions'][2],\n",
    "                \"BLEU_4\": bleu_result['precisions'][3]\n",
    "            }\n",
    "            print(f\"Test result: val loss={avg_val_loss}, BLEU={result['avg']}, BLEU1={result['bleu-1']}, BLEU2={result['bleu-2']}, BLEU3={result['bleu-3']}, BLEU4={result['bleu-4']}\")\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8bd5c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bleu/9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76 (last modified on Mon Feb  2 11:38:55 2026) since it couldn't be found locally at evaluate-metric--bleu, or remotely on the Hugging Face Hub.\n",
      "100%|██████████| 43/43 [00:23<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: val loss=3.061885869780252, BLEU=0.34754358423024007, BLEU1=0.6637407157326131, BLEU2=0.5064710957722174, BLEU3=0.41327124563445866, BLEU4=0.34185303514376997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu-1': 0.6637407157326131,\n",
       " 'bleu-2': 0.5064710957722174,\n",
       " 'bleu-3': 0.41327124563445866,\n",
       " 'bleu-4': 0.34185303514376997,\n",
       " 'avg': 0.34754358423024007}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loop(test_dataloader, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd23912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  问题:qsv是什么格式 上下文:qsv格式是爱奇艺研发的一种视频格式,只能使用奇艺播放器播放。这太不方便了,我想在手机里观看还要下载你的应用。拷到其他电脑上也要下载你的客户端。不过可以将其转为其他格式的视频文件文件,让各种播放器都可以播放。\n",
      "label:  视频\n",
      "------\n",
      "Input:  问题:华为p10 销量 上下文:中关村在线消息:昨天下午华为在上海发布了旗舰系列手机P10/P10 Plus,华为最低版本P10(4GB+64GB)售价3788元,4GB+128GB版本的4288元,华为P10 Plus的6GB+64GB版售价4388元,6GB+128GB版的为4888元,最高配的6GB+256GB版售价则达到5588元,这是国产旗舰系列手机首次超过5000元档。|与在MWC2017发布时在欧洲的价格对比看,3788元起还算厚道,只是比华为自家的Mate系列比又贵了一些。很多人会问,同样是麒麟960处理器,同样是徕卡双摄,P10为何更贵一些?|对比,华为消费业务CEO余承东表示手机重要元器件成本上升是P10系列涨价的一个因素,另一方面,华为砍掉了P系列中的“低配”,即去掉了32GB容量的版本,直接从64GB存储容量开始。|第三,作为华为手机业务的长期合作伙伴,徕卡在华为手机中扮演着重要的角色。余承东也坦言,每台P10手机的利润徕卡都有份,只是具体多少不便透露。除了摄像头,华为P10系列的钻雕工艺也花了重金打造,以突出时尚感。|昨天18:08分起,华为P10系列手机已经在线上开启预约、线下抢先发售。截至凌晨12:00,P10在京东商城的销量已破亿。可以说,这也是个不错的开始。|华为P10系列3788-5588元售价,使人免不了会拿它与苹果相比,对于与苹果的差别,余承东在发布会后接受媒体采访时表示,目前来看,苹果iPhone 7有的华为P10也有了,唯一的区别在于,苹果iPhone之所以强大,是因为它拥有一个良好的应用生态。而这也是华为未来的目标。|本文属于原创文章,如若转载,请注明来源:华为P10发布首日销量过亿 价格高成本涨http://mobile.zol.com.cn/632/6327283.html\n",
      "label:  到万\n",
      "------\n",
      "Input:  问题:中国共产党成立时间 上下文:中国共产党,简称中共,成立于1921年7月,1949年10月至今为代表工人阶级领导工农联盟和统一战线,在中国大陆实行人民民主专政的中华人民共和国唯一执政党。中国工人阶级的先锋队,中国社会主义事业的领导核心,中国各族人民利益的代表者。中国共产党以马克思列宁主义、毛泽东思想、邓小平理论、“三个代表”重要思想和科学发展观作为自己的行动指南,最终目的是实现共产主义的社会制度。\n",
      "label:  2020年7月\n",
      "------\n",
      "Input:  问题:龙珠传奇之无间道什么时候播 上下文:龙珠传奇之无间道播出时间电视剧《龙珠传奇之无间道》将于2017年05月08日起在安徽卫视上映播出龙珠传奇之无间道在哪个台播什么时候播更新时间安徽卫视;;; 一天更新两集每周一至周三晚22:00更新北京卫视;;;;一天更新两集每周一至周三晚22:00更新龙珠传奇之无间道网络更新时间优酷;;;;每周一至周三24点更新VIP会员于每周日00:00抢先看12集腾讯;;;;每周一至周三24点更新VIP会员于每周日00:00抢先看12集周远舟执导的电视剧《龙珠传奇之无间道》将于2017年05月08日起在安徽卫视上映播出,该剧由杨紫(李易欢/朱易欢)、秦俊杰(康熙)、舒畅(雪倾城/舒婉心)、茅子俊(朱慈煊/李剑卿)、孙蔚(樊倩影)等主演。《龙珠传奇之无间道》这部清装传奇大戏五月中旬在横店开拍导演周远舟拍过《麻雀》《诛仙青云志》《乱世佳人》《烽火佳人》等由于《龙珠传奇》系列的故事架构宏达人物众多每季身份关系以及情感翻转力度极大是一部强情节快节奏绝不拖泥带水的烧脑大剧堪称中国历史非玄幻版《权力的游戏》该剧主要是讲述了明朝最后一个公主朱易欢与少年康熙从欢喜冤家到两心相许再到不得不反目为仇、并最终放弃了复仇和康熙约定相忘于世相记于心的故事。该剧将打造成大IP预计拍摄四季《龙珠传奇之无间道》、《龙珠传奇之修罗道》、《龙珠传奇之人间道》、《龙珠传奇之天道》。2016年先拍第一季2017年初拍第二季2017年底拍第三季最后一季在2018年开拍。《龙珠传奇之无间道》这部电视剧主要是围绕着明朝最后一个公主朱易欢与少年康熙从欢喜冤家到两心相许再到不得不反目为仇、并最终放弃了复仇和康熙约定相忘于世相记于心的故事。\n",
      "label:  2017年05月12日\n",
      "------\n",
      "Input:  问题:苏州平均工资 上下文:近日,一份2017年苏州的薪资水平报告出来了,苏州的薪资水平平均在4755元,而且据说这份报告是对33055份的样本数据分析而得出的。|从表格中我们可以看到,薪资在3k-4.5k占比21.6%,五千不到的平均工资在现在的苏州生活,可能只能用拮据来形容吧~\n",
      "label:  4550元\n",
      "------\n",
      "Input:  问题:dnf暗精灵遗迹在哪里 上下文:暗精灵遗迹在副职业导师的左边,首先你要完成就任务(暗精灵遗迹),然后走去亡者峡谷那条路过去(打各种塔,死亡之塔,迷茫之塔的地方),路上会有两个暗精灵一左一右的站在一扇门的左右边,那个就是暗精灵遗迹的入口\n",
      "label:  \n",
      "------\n",
      "Input:  问题:盾构机一台多少钱 上下文:一般直径的地铁盾构(也就是直径6米的)大约4000万左右(国内盾构生产厂商),国外的(如德国、美国)会稍微贵一些(日本可能会便宜一些)。大直径的盾构机会更贵,具体价格视盾构直径而定,当然土压平衡盾构和泥水平衡盾构的价钱也不一样,简单来说泥水平衡盾构会比相同直径的土压平衡盾构贵一些。\n",
      "label:  2000万\n",
      "------\n",
      "Input:  问题:杭州治疗青春痘多少钱 上下文:医院的选择上面由于每家医院的医疗水平不同配套的诊疗设备以及医护人员的水平不一样所以在收费方面也自然不一样。而且不同地域的消费水平不一样也导致了不同地域医院的青春痘收费标准不一样。一般所需费用在几十到几百之间。我是去的格莱美看的。挺不错的。楼主可以在网上了解一下\n",
      "label:  几千到几千\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "test_data = next(iter(test_dataloader))\n",
    "batch_data = test_data.to(device)\n",
    "outputs = model.generate(\n",
    "    batch_data[\"input_ids\"],\n",
    "    attention_mask=batch_data[\"attention_mask\"],\n",
    "    max_new_tokens=MAX_TARGET_LENGTH,\n",
    "    num_beams=4\n",
    "    )\n",
    "decoded_outputs = tokenizer.batch_decode(\n",
    "    outputs,\n",
    "    skip_special_tokens=True\n",
    "    )\n",
    "for input, label in zip(\n",
    "    tokenizer.batch_decode( batch_data[\"input_ids\"], skip_special_tokens=True), \n",
    "    decoded_outputs):\n",
    "    print(\"Input: \",input )\n",
    "    print(\"label: \", label)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3785a499",
   "metadata": {},
   "source": [
    "# Model with 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "438fc71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:10<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: val loss=2.2155101105224255, BLEU=0.580597525929668, BLEU1=0.7590630228667038, BLEU2=0.6499312242090785, BLEU3=0.577639751552795, BLEU4=0.5269953051643192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bleu-1': 0.7590630228667038,\n",
       " 'bleu-2': 0.6499312242090785,\n",
       " 'bleu-3': 0.577639751552795,\n",
       " 'bleu-4': 0.5269953051643192,\n",
       " 'avg': 0.580597525929668}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldername =  '31-01-26-15_14_more_ckpt'\n",
    "checkpoint_path = Path(f\"./checkpoint/{foldername}\")\n",
    "file_path = checkpoint_path / best_model_name\n",
    "\n",
    "model.load_state_dict(torch.load(file_path, weights_only=True))\n",
    "\n",
    "test_loop(test_dataloader, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f0f743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  问题:qsv是什么格式 上下文:qsv格式是爱奇艺研发的一种视频格式,只能使用奇艺播放器播放。这太不方便了,我想在手机里观看还要下载你的应用。拷到其他电脑上也要下载你的客户端。不过可以将其转为其他格式的视频文件文件,让各种播放器都可以播放。\n",
      "label:  视频格式\n",
      "------\n",
      "Input:  问题:华为p10 销量 上下文:中关村在线消息:昨天下午华为在上海发布了旗舰系列手机P10/P10 Plus,华为最低版本P10(4GB+64GB)售价3788元,4GB+128GB版本的4288元,华为P10 Plus的6GB+64GB版售价4388元,6GB+128GB版的为4888元,最高配的6GB+256GB版售价则达到5588元,这是国产旗舰系列手机首次超过5000元档。|与在MWC2017发布时在欧洲的价格对比看,3788元起还算厚道,只是比华为自家的Mate系列比又贵了一些。很多人会问,同样是麒麟960处理器,同样是徕卡双摄,P10为何更贵一些?|对比,华为消费业务CEO余承东表示手机重要元器件成本上升是P10系列涨价的一个因素,另一方面,华为砍掉了P系列中的“低配”,即去掉了32GB容量的版本,直接从64GB存储容量开始。|第三,作为华为手机业务的长期合作伙伴,徕卡在华为手机中扮演着重要的角色。余承东也坦言,每台P10手机的利润徕卡都有份,只是具体多少不便透露。除了摄像头,华为P10系列的钻雕工艺也花了重金打造,以突出时尚感。|昨天18:08分起,华为P10系列手机已经在线上开启预约、线下抢先发售。截至凌晨12:00,P10在京东商城的销量已破亿。可以说,这也是个不错的开始。|华为P10系列3788-5588元售价,使人免不了会拿它与苹果相比,对于与苹果的差别,余承东在发布会后接受媒体采访时表示,目前来看,苹果iPhone 7有的华为P10也有了,唯一的区别在于,苹果iPhone之所以强大,是因为它拥有一个良好的应用生态。而这也是华为未来的目标。|本文属于原创文章,如若转载,请注明来源:华为P10发布首日销量过亿 价格高成本涨http://mobile.zol.com.cn/632/6327283.html\n",
      "label:  超过亿\n",
      "------\n",
      "Input:  问题:中国共产党成立时间 上下文:中国共产党,简称中共,成立于1921年7月,1949年10月至今为代表工人阶级领导工农联盟和统一战线,在中国大陆实行人民民主专政的中华人民共和国唯一执政党。中国工人阶级的先锋队,中国社会主义事业的领导核心,中国各族人民利益的代表者。中国共产党以马克思列宁主义、毛泽东思想、邓小平理论、“三个代表”重要思想和科学发展观作为自己的行动指南,最终目的是实现共产主义的社会制度。\n",
      "label:  1921年7月\n",
      "------\n",
      "Input:  问题:龙珠传奇之无间道什么时候播 上下文:龙珠传奇之无间道播出时间电视剧《龙珠传奇之无间道》将于2017年05月08日起在安徽卫视上映播出龙珠传奇之无间道在哪个台播什么时候播更新时间安徽卫视;;; 一天更新两集每周一至周三晚22:00更新北京卫视;;;;一天更新两集每周一至周三晚22:00更新龙珠传奇之无间道网络更新时间优酷;;;;每周一至周三24点更新VIP会员于每周日00:00抢先看12集腾讯;;;;每周一至周三24点更新VIP会员于每周日00:00抢先看12集周远舟执导的电视剧《龙珠传奇之无间道》将于2017年05月08日起在安徽卫视上映播出,该剧由杨紫(李易欢/朱易欢)、秦俊杰(康熙)、舒畅(雪倾城/舒婉心)、茅子俊(朱慈煊/李剑卿)、孙蔚(樊倩影)等主演。《龙珠传奇之无间道》这部清装传奇大戏五月中旬在横店开拍导演周远舟拍过《麻雀》《诛仙青云志》《乱世佳人》《烽火佳人》等由于《龙珠传奇》系列的故事架构宏达人物众多每季身份关系以及情感翻转力度极大是一部强情节快节奏绝不拖泥带水的烧脑大剧堪称中国历史非玄幻版《权力的游戏》该剧主要是讲述了明朝最后一个公主朱易欢与少年康熙从欢喜冤家到两心相许再到不得不反目为仇、并最终放弃了复仇和康熙约定相忘于世相记于心的故事。该剧将打造成大IP预计拍摄四季《龙珠传奇之无间道》、《龙珠传奇之修罗道》、《龙珠传奇之人间道》、《龙珠传奇之天道》。2016年先拍第一季2017年初拍第二季2017年底拍第三季最后一季在2018年开拍。《龙珠传奇之无间道》这部电视剧主要是围绕着明朝最后一个公主朱易欢与少年康熙从欢喜冤家到两心相许再到不得不反目为仇、并最终放弃了复仇和康熙约定相忘于世相记于心的故事。\n",
      "label:  2017年05月05日\n",
      "------\n",
      "Input:  问题:苏州平均工资 上下文:近日,一份2017年苏州的薪资水平报告出来了,苏州的薪资水平平均在4755元,而且据说这份报告是对33055份的样本数据分析而得出的。|从表格中我们可以看到,薪资在3k-4.5k占比21.6%,五千不到的平均工资在现在的苏州生活,可能只能用拮据来形容吧~\n",
      "label:  4555元\n",
      "------\n",
      "Input:  问题:dnf暗精灵遗迹在哪里 上下文:暗精灵遗迹在副职业导师的左边,首先你要完成就任务(暗精灵遗迹),然后走去亡者峡谷那条路过去(打各种塔,死亡之塔,迷茫之塔的地方),路上会有两个暗精灵一左一右的站在一扇门的左右边,那个就是暗精灵遗迹的入口\n",
      "label:  \n",
      "------\n",
      "Input:  问题:盾构机一台多少钱 上下文:一般直径的地铁盾构(也就是直径6米的)大约4000万左右(国内盾构生产厂商),国外的(如德国、美国)会稍微贵一些(日本可能会便宜一些)。大直径的盾构机会更贵,具体价格视盾构直径而定,当然土压平衡盾构和泥水平衡盾构的价钱也不一样,简单来说泥水平衡盾构会比相同直径的土压平衡盾构贵一些。\n",
      "label:  4000万左右\n",
      "------\n",
      "Input:  问题:杭州治疗青春痘多少钱 上下文:医院的选择上面由于每家医院的医疗水平不同配套的诊疗设备以及医护人员的水平不一样所以在收费方面也自然不一样。而且不同地域的消费水平不一样也导致了不同地域医院的青春痘收费标准不一样。一般所需费用在几十到几百之间。我是去的格莱美看的。挺不错的。楼主可以在网上了解一下\n",
      "label:  几百到几百\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "test_data = next(iter(test_dataloader))\n",
    "batch_data = test_data.to(device)\n",
    "outputs = model.generate(\n",
    "    batch_data[\"input_ids\"],\n",
    "    attention_mask=batch_data[\"attention_mask\"],\n",
    "    max_new_tokens=MAX_TARGET_LENGTH,\n",
    "    num_beams=4\n",
    "    )\n",
    "decoded_outputs = tokenizer.batch_decode(\n",
    "    outputs,\n",
    "    skip_special_tokens=True\n",
    "    )\n",
    "for input, label in zip(\n",
    "    tokenizer.batch_decode( batch_data[\"input_ids\"], skip_special_tokens=True), \n",
    "    decoded_outputs):\n",
    "    print(\"Input: \",input )\n",
    "    print(\"label: \", label)\n",
    "    print(\"------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
