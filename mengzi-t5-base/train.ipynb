{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e96d5b",
   "metadata": {},
   "source": [
    "# Call library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e51419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lamye\\miniconda3\\envs\\transformers\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import torch\n",
    "import os\n",
    "import evaluate \n",
    "import wandb\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, get_scheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from utils import save_checkpoint, read_json, get_data_stats, collote_fn, MAX_TARGET_LENGTH\n",
    "from dataset import MengziT5Dataset\n",
    "from pathlib import Path\n",
    "from datetime import datetime \n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "checkpoint = \"Langboat/mengzi-t5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b4671",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "679b5ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Reading JSON file: 984it [00:00, 92279.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First valid data:  {'context': '年基准利率4.35%。 从实际看,贷款的基本条件是: 一是中国大陆居民,年龄在60岁以下; 二是有稳定的住址和工作或经营地点; 三是有稳定的收入来源; 四是无不良信用记录,贷款用途不能作为炒股,赌博等行为; 五是具有完全民事行为能力。', 'answer': '年基准利率4.35%', 'question': '2017年银行贷款基准利率', 'id': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSON file: 14520it [00:00, 72396.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train data:  {'context': '第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。', 'answer': '第35集', 'question': '仙剑奇侠传3第几集上天界', 'id': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = \"data/train.json\"\n",
    "DATA_DEV_PATH = \"data/dev.json\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint) \n",
    "\n",
    "valid_data = read_json(DATA_DEV_PATH)\n",
    "print(\"First valid data: \", valid_data[0])\n",
    "train_data = read_json(DATA_TRAIN_PATH)\n",
    "print(\"First train data: \", train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8fec6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_num': 984,\n",
       " 'context_num': 984,\n",
       " 'answer_num': 984,\n",
       " 'question_mean_length': 5.616869918699187,\n",
       " 'context_mean_length': 191.1971544715447,\n",
       " 'answer_mean_length': 3.9390243902439024,\n",
       " 'question_max_length': 17,\n",
       " 'context_max_length': 727,\n",
       " 'answer_max_length': 25}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_stats(valid_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09942fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_num': 14520,\n",
       " 'context_num': 14520,\n",
       " 'answer_num': 14520,\n",
       " 'question_mean_length': 5.561776859504132,\n",
       " 'context_mean_length': 181.33471074380165,\n",
       " 'answer_mean_length': 3.443595041322314,\n",
       " 'question_max_length': 27,\n",
       " 'context_max_length': 1176,\n",
       " 'answer_max_length': 94}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_stats(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9388d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data filtered away: 165\n",
      "Total data filtered away: 1906\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = MengziT5Dataset(valid_data)\n",
    "train_dataset = MengziT5Dataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa0fd0",
   "metadata": {},
   "source": [
    "# Retrieve Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9497a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "valid_batch_size = 8\n",
    "test_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30dc1aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input_ids:  tensor([[  143,    13,  7850,  ...,     0,     0,     0],\n",
      "        [  143,    13,  1058,  ...,     0,     0,     0],\n",
      "        [  143,    13,  2536,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  143,    13,   875,  ...,     0,     0,     0],\n",
      "        [  143,    13, 29112,  ...,     0,     0,     0],\n",
      "        [  143,    13,   900,  ...,     0,     0,     0]])\n",
      "train attention_mask:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "train decoder_input_ids tensor([[    0, 10813, 24736,  ...,     0,     0,     0],\n",
      "        [    0, 10603,     1,  ...,     0,     0,     0],\n",
      "        [    0,  2056,  2823,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0,  5707, 15389,  ...,     0,     0,     0],\n",
      "        [    0,  1258,   212,  ...,     0,     0,     0],\n",
      "        [    0,  9395,     1,  ...,     0,     0,     0]])\n",
      "train labels tensor([[10813, 24736,  8115,  ...,  -100,  -100,  -100],\n",
      "        [10603,     1,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ 2056,  2823,  2835,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [ 5707, 15389,   995,  ...,  -100,  -100,  -100],\n",
      "        [ 1258,   212,     1,  ...,  -100,  -100,  -100],\n",
      "        [ 9395,     1,  -100,  ...,  -100,  -100,  -100]])\n",
      "----------\n",
      "valid input_ids:  tensor([[ 143,   13, 4254,  ...,    0,    0,    0],\n",
      "        [ 143,   13, 7361,  ...,    0,    0,    0],\n",
      "        [ 143,   13,   87,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 143,   13, 4920,  ...,    0,    0,    0],\n",
      "        [ 143,   13, 5961,  ...,    0,    0,    0],\n",
      "        [ 143,   13, 8607,  ...,    0,    0,    0]])\n",
      "valid attention_mask:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "valid decoder_input_ids:  tensor([[    0,   218,   880,  ...,     0,     0,     0],\n",
      "        [    0,  4228,  2142,  ...,     0,     0,     0],\n",
      "        [    0,    70, 25236,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0,   105,   976,  ...,     0,     0,     0],\n",
      "        [    0, 22061,   840,  ...,     0,     0,     0],\n",
      "        [    0,  9787, 13431,  ...,     0,     0,     0]])\n",
      "valid labels: tensor([[  218,   880,  6466,  ...,  -100,  -100,  -100],\n",
      "        [ 4228,  2142,  4290,  ...,  -100,  -100,  -100],\n",
      "        [   70, 25236,   624,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [  105,   976,  2312,  ...,  -100,  -100,  -100],\n",
      "        [22061,   840,     1,  ...,  -100,  -100,  -100],\n",
      "        [ 9787, 13431,     1,  ...,  -100,  -100,  -100]])\n",
      "test input_ids:  tensor([[  143,    13,  3563,  ...,     0,     0,     0],\n",
      "        [  143,    13,   712,  ...,     0,     0,     0],\n",
      "        [  143,    13, 15026,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  143,    13,  2906,  ...,     0,     0,     0],\n",
      "        [  143,    13,   195,  ...,     0,     0,     0],\n",
      "        [  143,    13,  2796,  ...,     0,     0,     0]])\n",
      "test attention_mask:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "test decoder_input_ids:  tensor([[    0,   419,   236,  ...,     0,     0,     0],\n",
      "        [    0,  4185,   126,  ...,     0,     0,     0],\n",
      "        [    0, 12091, 13687,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0,    99,   149,  ...,     0,     0,     0],\n",
      "        [    0,  7676,  4794,  ...,     0,     0,     0],\n",
      "        [    0, 12950,  5765,  ...,     0,     0,     0]])\n",
      "test labels: tensor([[  419,   236,   552,  ...,  -100,  -100,  -100],\n",
      "        [ 4185,   126, 10949,  ...,  -100,  -100,  -100],\n",
      "        [12091, 13687, 11418,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [   99,   149,     1,  ...,  -100,  -100,  -100],\n",
      "        [ 7676,  4794,    38,  ...,  -100,  -100,  -100],\n",
      "        [12950,  5765,   229,  ...,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size, collate_fn=lambda x: collote_fn(x, model, tokenizer))\n",
    "train_data = next(iter(train_dataloader))\n",
    "print(\"train input_ids: \", train_data['input_ids'])\n",
    "print(\"train attention_mask: \", train_data['attention_mask'])\n",
    "print(\"train decoder_input_ids\", train_data['decoder_input_ids'])\n",
    "print(\"train labels\", train_data['labels'])\n",
    "print(\"----------\")\n",
    "\n",
    "valid_dataset, test_dataset = random_split(valid_dataset, [0.5, 0.5])\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=valid_batch_size, collate_fn=lambda x: collote_fn(x, model, tokenizer))\n",
    "valid_data = next(iter(valid_dataloader))\n",
    "print(\"valid input_ids: \", valid_data['input_ids'])\n",
    "print(\"valid attention_mask: \", valid_data['attention_mask'])\n",
    "print(\"valid decoder_input_ids: \", valid_data['decoder_input_ids'])\n",
    "print(\"valid labels:\", valid_data['labels'])\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=valid_batch_size, collate_fn=lambda x: collote_fn(x, model, tokenizer))\n",
    "test_data = next(iter(test_dataloader))\n",
    "print(\"test input_ids: \", test_data['input_ids'])\n",
    "print(\"test attention_mask: \", test_data['attention_mask'])\n",
    "print(\"test decoder_input_ids: \", test_data['decoder_input_ids'])\n",
    "print(\"test labels:\", test_data['labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917848ee",
   "metadata": {},
   "source": [
    "# Train Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer, scheduler, epoch, total_loss, use_wandb=False):\n",
    "    cumulative_batch = (epoch-1) * len(dataloader)\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        pbar.set_description(f\"Starting {epoch} epoch. Current loss: {total_loss / cumulative_batch}\")\n",
    "        for batch_idx, batch_data in enumerate(dataloader, start=1):\n",
    "            batch_data = batch_data.to(device)\n",
    "            results = model(**batch_data)\n",
    "            loss = results.loss\n",
    "\n",
    "            # backward popagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    {\"train_loss\": loss.item()},\n",
    "                    step=cumulative_batch + batch_idx\n",
    "                )\n",
    "            total_loss += loss # Sum to loss from the first epoch of first batch\n",
    "\n",
    "            pbar.set_description(f\"Cumulative loss: {total_loss / (cumulative_batch + batch_idx):>7f}\")\n",
    "            pbar.update(1)\n",
    "    return total_loss\n",
    "\n",
    "def valid_loop(dataloader, model, tokenizer, epoch, use_wandb=False):\n",
    "    model.eval()\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    cumulative_batch = (epoch-1) * len(dataloader)\n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(dataloader, start=1):\n",
    "                batch_data = batch_data.to(device)\n",
    "                results = model(**batch_data)\n",
    "                loss = results.loss\n",
    "\n",
    "                if use_wandb:\n",
    "                    wandb.log(\n",
    "                        {\"valid_loss\": loss.item()},\n",
    "                        step=cumulative_batch + batch_idx\n",
    "                    )\n",
    "\n",
    "                if batch_idx < 3:\n",
    "                    outputs = model.generate(\n",
    "                        batch_data[\"input_ids\"],\n",
    "                        attention_mask=batch_data[\"attention_mask\"],\n",
    "                        max_new_token=MAX_TARGET_LENGTH,\n",
    "                        num_beams=4\n",
    "                        )\n",
    "                    decoded_outputs = tokenizer.batch_decode(\n",
    "                        outputs,\n",
    "                        skip_special_tokens=True\n",
    "                        )\n",
    "                    labels = batch_data['labels']\n",
    "                    labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "                    decoded_labels = tokenizer.batch_decode(\n",
    "                        labels,\n",
    "                        skip_special_tokens=True\n",
    "                    )\n",
    "\n",
    "                    preds = [' '.join(pred.strip()) for pred in decoded_outputs]\n",
    "                    labels = [' '.join(label.strip()) for label in decoded_labels]\n",
    "            bleu_result = bleu.compute(predictions=preds, references=labels)\n",
    "            result = {f\"bleu-{i}\" : value for i, value in enumerate(bleu_result[\"precisions\"], start=1)}\n",
    "            result['avg'] = np.mean(result.values())\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    {\"BLEU_avg\": result['avg']},\n",
    "                    step=epoch * len(dataloader)\n",
    "                )\n",
    "            print(f\"Test result: BLEU1={result['bleu-1']}, BLEU2={result['bleu-2']}, BLEU3={result['bleu-3']}, BLEU4={result['bleu-4']}\")\n",
    "            return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5366ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting 3 epoch. Current loss: 0.0:   0%|          | 0/1577 [01:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Seq2SeqLMOutput' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m best_bleu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_num):\n\u001b[1;32m---> 36\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_wandb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     valid_bleu \u001b[38;5;241m=\u001b[39m valid_loop(valid_dataloader, model, use_wandb\u001b[38;5;241m=\u001b[39muse_wandb)\n\u001b[0;32m     38\u001b[0m     bleu_avg \u001b[38;5;241m=\u001b[39m valid_bleu[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(dataloader, model, optimizer, scheduler, epoch, total_loss, use_wandb)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# backward popagation\u001b[39;00m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     16\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Seq2SeqLMOutput' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-5\n",
    "epoch_num = 3\n",
    "best_model_name = \"best_t5.pt\"\n",
    "current_t = datetime.now().strftime('%d-%m-%y-%H_%M')\n",
    "foldername =  current_t + '_ckpt.pth'\n",
    "checkpoint_path = Path(f\"./checkpoint/{foldername}\")\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "file_path = checkpoint_path / best_model_name\n",
    "recent_checkpoints = []\n",
    "use_wandb = False\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"mengzi-t5-qa\",   # The name of your project on the website\n",
    "        name=f\"{current_t}\",  # Name of this specific training run\n",
    "        config={                  # Save hyperparameters for reference\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": train_batch_size,\n",
    "            \"epochs\": epoch_num,\n",
    "            \"model\": \"mengzi-t5-base\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "num_training_steps = epoch_num * len(train_dataloader)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "total_loss = 0\n",
    "best_bleu = 0\n",
    "for epoch in range(epoch_num):\n",
    "    total_loss = train_loop(train_dataloader, model, optimizer, scheduler, epoch_num, total_loss, use_wandb=use_wandb)\n",
    "    valid_bleu = valid_loop(valid_dataloader, model, use_wandb=use_wandb)\n",
    "    bleu_avg = valid_bleu['avg']\n",
    "    save_checkpoint(model, epoch, checkpoint_path, recent_checkpoints)\n",
    "    if bleu_avg > best_bleu:\n",
    "        best_bleu = bleu_avg \n",
    "        print(\"Saving new best weights ...\")\n",
    "        torch.save(model.static_dict() , file_path)\n",
    "        print(\"Finish saving.\")\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    \n",
    "\n",
    "print(\"Finish training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
