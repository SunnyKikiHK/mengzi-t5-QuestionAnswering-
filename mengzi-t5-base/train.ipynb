{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e96d5b",
   "metadata": {},
   "source": [
    "# Call library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e51419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import torch\n",
    "import os\n",
    "import evaluate \n",
    "import wandb\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, get_scheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from utils import save_checkpoint, read_json, get_data_stats, collote_train_fn, collote_valid_fn, merge_qa_dataset, MAX_TARGET_LENGTH\n",
    "from dataset import MengziT5Dataset\n",
    "from pathlib import Path\n",
    "from datetime import datetime \n",
    "from tqdm import tqdm \n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "checkpoint = \"Langboat/mengzi-t5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b4671",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679b5ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSON file: 984it [00:00, 139253.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 984 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing to JSON file: 100%|██████████| 700/700 [00:00<00:00, 87986.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Merged data saved to data/formatted_dev.json\n",
      "Original count: 984 -> New count: 700\n",
      "First valid data:  {'id': 0, 'context': '年基准利率4.35%。 从实际看,贷款的基本条件是: 一是中国大陆居民,年龄在60岁以下; 二是有稳定的住址和工作或经营地点; 三是有稳定的收入来源; 四是无不良信用记录,贷款用途不能作为炒股,赌博等行为; 五是具有完全民事行为能力。', 'question': '2017年银行贷款基准利率', 'answer': ['4.35%', '年基准利率4.35%']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSON file: 14520it [00:00, 158061.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train data:  {'context': '第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。', 'answer': '第35集', 'question': '仙剑奇侠传3第几集上天界', 'id': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = \"data/train.json\"\n",
    "DATA_DEV_PATH = \"data/dev.json\"\n",
    "\n",
    "DATA_FDEV_PATH = \"data/formatted_dev.json\"\n",
    "DATA_DEV_PATH = \"data/dev.json\"\n",
    "\n",
    "valid_data = read_json(DATA_DEV_PATH)\n",
    "merged_valid_data = merge_qa_dataset(valid_data, DATA_FDEV_PATH)\n",
    "# merged_valid_data = read_json(DATA_FDEV_PATH)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint) \n",
    "\n",
    "print(\"First valid data: \", merged_valid_data[0])\n",
    "train_data = read_json(DATA_TRAIN_PATH)\n",
    "print(\"First train data: \", train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8fec6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_num': 984,\n",
       " 'context_num': 984,\n",
       " 'answer_num': 984,\n",
       " 'question_mean_length': 6.5426829268292686,\n",
       " 'context_mean_length': 192.15243902439025,\n",
       " 'answer_mean_length': 4.774390243902439,\n",
       " 'question_max_length': 18,\n",
       " 'context_max_length': 728,\n",
       " 'answer_max_length': 26}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_stats(valid_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09942fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_num': 14520,\n",
       " 'context_num': 14520,\n",
       " 'answer_num': 14520,\n",
       " 'question_mean_length': 6.488154269972452,\n",
       " 'context_mean_length': 182.3798209366391,\n",
       " 'answer_mean_length': 4.257782369146006,\n",
       " 'question_max_length': 28,\n",
       " 'context_max_length': 1180,\n",
       " 'answer_max_length': 95}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_stats(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9388d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data filtered away: 19\n",
      "Total data filtered away: 538\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = MengziT5Dataset(merged_valid_data, tokenizer)\n",
    "train_dataset = MengziT5Dataset(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa0fd0",
   "metadata": {},
   "source": [
    "# Retrieve Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9497a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "valid_batch_size = 8\n",
    "#test_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30dc1aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 282/282 [00:00<00:00, 515.16it/s, Materializing param=shared.weight]                                                       \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input_ids:  tensor([[  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0]])\n",
      "train attention_mask:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "train decoder_input_ids tensor([[    0,     7,   552,  ...,     0,     0,     0],\n",
      "        [    0,     7,  4864,  ...,     0,     0,     0],\n",
      "        [    0,     7, 23543,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0,     7, 19564,  ...,     0,     0,     0],\n",
      "        [    0,     7,  1590,  ...,     0,     0,     0],\n",
      "        [    0,     7,   318,  ...,     0,     0,     0]])\n",
      "train labels tensor([[    7,   552,  1236,  ...,  -100,  -100,  -100],\n",
      "        [    7,  4864,   197,  ...,  -100,  -100,  -100],\n",
      "        [    7, 23543,  1364,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [    7, 19564,    24,  ...,  -100,  -100,  -100],\n",
      "        [    7,  1590,  1276,  ...,  -100,  -100,  -100],\n",
      "        [    7,   318,  1930,  ...,  -100,  -100,  -100]])\n",
      "----------\n",
      "valid input_ids:  tensor([[  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0]])\n",
      "valid attention_mask:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "valid decoder_input_ids:  tensor([[    0,     7,   419,  ...,     0,     0,     0],\n",
      "        [    0, 12598,  9787,  ...,     0,     0,     0],\n",
      "        [    0,  8811,    92,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0,     7,  3485,  ...,     0,     0,     0],\n",
      "        [    0,     7,   280,  ...,     0,     0,     0],\n",
      "        [    0, 11384,   707,  ...,     0,     0,     0]])\n",
      "valid labels: tensor([[    7,   419,   725,  ...,  -100,  -100,  -100],\n",
      "        [12598,  9787,   101,  ...,  -100,  -100,  -100],\n",
      "        [ 8811,    92,     1,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [    7,  3485,     1,  ...,  -100,  -100,  -100],\n",
      "        [    7,   280, 15245,  ...,  -100,  -100,  -100],\n",
      "        [11384,   707,   296,  ...,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size, collate_fn=lambda x: collote_train_fn(x, model, tokenizer))\n",
    "train_data = next(iter(train_dataloader))\n",
    "print(\"train input_ids: \", train_data['input_ids'])\n",
    "print(\"train attention_mask: \", train_data['attention_mask'])\n",
    "print(\"train decoder_input_ids\", train_data['decoder_input_ids'])\n",
    "print(\"train labels\", train_data['labels'])\n",
    "print(\"----------\")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "valid_dataset, _ = random_split(valid_dataset, [0.5, 0.5], generator=generator)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=valid_batch_size, collate_fn=lambda x: collote_valid_fn(x, model, tokenizer))\n",
    "valid_data = next(iter(valid_dataloader))\n",
    "print(\"valid input_ids: \", valid_data['input_ids'])\n",
    "print(\"valid attention_mask: \", valid_data['attention_mask'])\n",
    "print(\"valid decoder_input_ids: \", valid_data['decoder_input_ids'])\n",
    "print(\"valid labels:\", valid_data['labels'])\n",
    "\n",
    "# test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=valid_batch_size, collate_fn=lambda x: collote_fn(x, model, tokenizer))\n",
    "# test_data = next(iter(test_dataloader))\n",
    "# print(\"test input_ids: \", test_data['input_ids'])\n",
    "# print(\"test attention_mask: \", test_data['attention_mask'])\n",
    "# print(\"test decoder_input_ids: \", test_data['decoder_input_ids'])\n",
    "# print(\"test labels:\", test_data['labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917848ee",
   "metadata": {},
   "source": [
    "# Train Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7350c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer, scheduler, epoch, global_step, use_wandb=False):\n",
    "    model.train()\n",
    "    # Reset loss counter at the start of the epoch\n",
    "    epoch_loss_sum = 0.0 \n",
    "    current_avg_loss = 0.0\n",
    "    #cumulative_batch = len(dataloader) * (epoch - 1)\n",
    "    \n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        for batch_idx, batch_data in enumerate(dataloader, start=1):\n",
    "            batch_data = batch_data.to(device)\n",
    "            results = model(**batch_data)\n",
    "            loss = results.loss\n",
    "\n",
    "            # backward popagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    {\"train_loss\": loss.item()},\n",
    "                    step=global_step\n",
    "                )\n",
    "\n",
    "            epoch_loss_sum += loss.item()\n",
    "            current_avg_loss = epoch_loss_sum / batch_idx\n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch} | Avg Loss: {current_avg_loss:.4f}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "    return current_avg_loss, global_step \n",
    "\n",
    "def valid_loop(dataloader, model, tokenizer, epoch, global_step, use_wandb=False):\n",
    "    model.eval()\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    loss = []\n",
    "    val_loss_sum = 0.0\n",
    "\n",
    "    #cumulative_batch = (epoch-1) * len(dataloader)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(dataloader, start=1):\n",
    "                raw_references = batch_data.pop(\"answer\", None)\n",
    "                if raw_references is None:\n",
    "                    print(\"No raw reference is found. Now create based on labels.\")\n",
    "                    temp_labels = torch.where(batch_data[\"labels\"] != -100, batch_data[\"labels\"], tokenizer.pad_token_id)\n",
    "                    raw_references = [[ref] for ref in tokenizer.batch_decode(temp_labels, skip_special_tokens=True)]\n",
    "\n",
    "\n",
    "                batch_data = batch_data.to(device)\n",
    "                results = model(**batch_data)\n",
    "                loss = results.loss\n",
    "                val_loss_sum += loss.item() # Accumulate loss\n",
    "\n",
    "                outputs = model.generate(\n",
    "                    batch_data[\"input_ids\"],\n",
    "                    attention_mask=batch_data[\"attention_mask\"],\n",
    "                    max_new_tokens=MAX_TARGET_LENGTH,\n",
    "                    num_beams=4\n",
    "                    )\n",
    "                decoded_outputs = tokenizer.batch_decode(\n",
    "                    outputs,\n",
    "                    skip_special_tokens=True\n",
    "                    )\n",
    "                # labels = batch_data['labels']\n",
    "                # labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "                # decoded_labels = tokenizer.batch_decode(\n",
    "                #     labels,\n",
    "                #     skip_special_tokens=True\n",
    "                # )\n",
    "\n",
    "                batch_preds = []\n",
    "                for pred in decoded_outputs:\n",
    "                    if len(pred) == 0:\n",
    "                        pred = \" \" # Prevent divided by zero during calculation of BLEU\n",
    "                    pred = ' '.join(pred.strip()) # 'A B C' \n",
    "                    batch_preds.append(pred)\n",
    "                \n",
    "                batch_labels = []\n",
    "                for ref_list in raw_references: # ref_list: [ans1, ans2, ...]\n",
    "                    processed_ref_list = []\n",
    "                    for ref in ref_list:\n",
    "                        cleaned_ref = ref.strip()\n",
    "                        processed_ref_list.append(' '.join(cleaned_ref.strip()))\n",
    "                    batch_labels.append(processed_ref_list)\n",
    "\n",
    "                # batch_preds = [' '.join(pred.strip()) for pred in decoded_outputs]\n",
    "                # batch_labels = [' '.join(label.strip()) for label in decoded_labels]\n",
    "                if batch_idx < 3:\n",
    "                    print(f\"First data: decoded output: {decoded_outputs[0]}, ref: {raw_references[0]}\")\n",
    "                all_preds.extend(batch_preds)\n",
    "                all_labels.extend(batch_labels)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "            bleu_result = bleu.compute(predictions=all_preds, references=all_labels)\n",
    "            result = {f\"bleu-{i}\" : value for i, value in enumerate(bleu_result[\"precisions\"], start=1)}\n",
    "            result['avg'] = bleu_result['bleu']\n",
    "            avg_val_loss = val_loss_sum / len(dataloader)\n",
    "            log_dict = {\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"BLEU_avg\": bleu_result['bleu'], # 'bleu' is the avg in huggingface evaluate\n",
    "                \"BLEU_1\": bleu_result['precisions'][0],\n",
    "                \"BLEU_2\": bleu_result['precisions'][1],\n",
    "                \"BLEU_3\": bleu_result['precisions'][2],\n",
    "                \"BLEU_4\": bleu_result['precisions'][3],\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    log_dict,\n",
    "                    step=global_step\n",
    "                )\n",
    "            print(f\"Test result: BLEU_avg={result['avg']}, BLEU1={result['bleu-1']}, BLEU2={result['bleu-2']}, BLEU3={result['bleu-3']}, BLEU4={result['bleu-4']}\")\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b91c22",
   "metadata": {},
   "source": [
    "## First 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5366ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from WANDB_API_KEY.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlamyeungkong0108\u001b[0m (\u001b[33mlamyeungkong0108-the-hong-kong-university-of-science-and\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mengzi-t5-QuestionAnswering-/mengzi-t5-base/wandb/run-20260202_151418-ngc0ornc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa/runs/ngc0ornc' target=\"_blank\">02-02-26-15_14</a></strong> to <a href='https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa' target=\"_blank\">https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa/runs/ngc0ornc' target=\"_blank\">https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa/runs/ngc0ornc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Avg Loss: 5.9303: 100%|██████████| 1748/1748 [05:53<00:00,  4.95it/s]\n",
      "  2%|▏         | 1/43 [00:00<00:16,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data: decoded output: , ref: ['30分钟']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/43 [00:00<00:11,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data: decoded output: 30, ref: ['10010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:13<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: BLEU_avg=0.0009316550067468875, BLEU1=0.4477124183006536, BLEU2=0.33714285714285713, BLEU3=0.2840909090909091, BLEU4=0.3023255813953488\n",
      "Saving checkpoint to checkpoint/02-02-26-15_14_ckpt/ckpt-epoch0.pt\n",
      "Saving new best weights ...\n",
      "Finish saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 3.7906: 100%|██████████| 1748/1748 [05:52<00:00,  4.96it/s]\n",
      "  2%|▏         | 1/43 [00:00<00:10,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data: decoded output: 30分钟, ref: ['30分钟']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/43 [00:00<00:10,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data: decoded output: 10010, ref: ['10010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:18<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: BLEU_avg=0.1511958423416365, BLEU1=0.3871866295264624, BLEU2=0.2315238718116416, BLEU3=0.14518633540372672, BLEU4=0.08083560399636694\n",
      "Saving checkpoint to checkpoint/02-02-26-15_14_ckpt/ckpt-epoch1.pt\n",
      "Saving new best weights ...\n",
      "Finish saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 3.1953: 100%|██████████| 1748/1748 [05:52<00:00,  4.96it/s]\n",
      "  5%|▍         | 2/43 [00:00<00:08,  4.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data: decoded output: 30分钟, ref: ['30分钟']\n",
      "First data: decoded output: 10010, ref: ['10010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:21<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: BLEU_avg=0.24219620032162895, BLEU1=0.4398841139546113, BLEU2=0.2968660968660969, BLEU3=0.20833333333333334, BLEU4=0.13610888710968774\n",
      "Saving checkpoint to checkpoint/02-02-26-15_14_ckpt/ckpt-epoch2.pt\n",
      "Saving new best weights ...\n",
      "Finish saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Avg Loss: 2.8427: 100%|██████████| 1748/1748 [05:52<00:00,  4.96it/s]\n",
      "  2%|▏         | 1/43 [00:00<00:09,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data: decoded output: 30分钟, ref: ['30分钟']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/43 [00:00<00:11,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data: decoded output: 10010, ref: ['10010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:25<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: BLEU_avg=0.28678966428005037, BLEU1=0.5390581717451524, BLEU2=0.3913630229419703, BLEU3=0.2939189189189189, BLEU4=0.2139874739039666\n",
      "Saving checkpoint to checkpoint/02-02-26-15_14_ckpt/ckpt-epoch3.pt\n",
      "Saving new best weights ...\n",
      "Finish saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Avg Loss: 2.6590: 100%|██████████| 1748/1748 [05:52<00:00,  4.96it/s]\n",
      "  2%|▏         | 1/43 [00:00<00:09,  4.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data: decoded output: 30分钟, ref: ['30分钟']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2/43 [00:00<00:11,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First data: decoded output: 10010, ref: ['10010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:27<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: BLEU_avg=0.2981586082477662, BLEU1=0.489292364990689, BLEU2=0.348302300109529, BLEU3=0.255249343832021, BLEU4=0.18167701863354038\n",
      "Saving checkpoint to checkpoint/02-02-26-15_14_ckpt/ckpt-epoch4.pt\n",
      "Saving new best weights ...\n",
      "Finish saving.\n",
      "Finish training\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "epoch_num = 5\n",
    "best_model_name = \"best_t5.pt\"\n",
    "current_t = datetime.now().strftime('%d-%m-%y-%H_%M')\n",
    "foldername =  current_t + '_ckpt'\n",
    "checkpoint_path = Path(f\"./checkpoint/{foldername}\")\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "file_path = checkpoint_path / best_model_name\n",
    "recent_checkpoints = []\n",
    "use_wandb = True\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"mengzi-t5-qa\",   # The name of project on the website\n",
    "        name=f\"{current_t}\",  # Name of this specific training run\n",
    "        config={        \n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": train_batch_size,\n",
    "            \"epochs\": epoch_num,\n",
    "            \"model\": \"mengzi-t5-base\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "num_training_steps = epoch_num * len(train_dataloader)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "global_step = 0\n",
    "best_bleu = 0\n",
    "for epoch in range(epoch_num):\n",
    "    avg_loss, global_step = train_loop(train_dataloader, model, optimizer, scheduler, epoch, global_step, use_wandb=use_wandb)\n",
    "    valid_bleu = valid_loop(valid_dataloader, model, tokenizer, epoch, global_step, use_wandb=use_wandb)\n",
    "    bleu_avg = valid_bleu['avg']\n",
    "    save_checkpoint(model, epoch, checkpoint_path, recent_checkpoints)\n",
    "    if bleu_avg > best_bleu:\n",
    "        best_bleu = bleu_avg \n",
    "        print(\"Saving new best weights ...\")\n",
    "        torch.save(model.state_dict() , file_path)\n",
    "        print(\"Finish saving.\")\n",
    "    \n",
    "\n",
    "print(\"Finish training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3bb3a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  问题:慢跑多久开始燃烧脂肪 上下文:慢跑30分钟才开始燃烧脂肪。美国运动协会进行了一项研讨在受试者手臂植入探测器开端运动后血糖在榜首分钟开端耗费运动10分钟后脂肪组织中的血流量添加表明脂肪开端焚烧脂肪组织血流量在运动30分钟时到达最高。即便中止运动脂肪组织血流量最高浓度仍可继续6小时。脂肪由甘油和脂肪酸组成研讨一起剖析受试者血液发现其间甘油和游离脂肪酸添加表明脂肪开端分化。依据研讨结果科学家主张有心使用运动减重者最佳趁热打铁接连30分钟如此就能焚烧脂肪6小时。以耗费热量来说接连运动和“分步走”耗费的热量是一样的但若想焚烧更多的脂肪最佳仍是坚持一下一次就接连运动30分钟脂肪就能焚烧6小时作用最佳。可是运动时刻也不必太多研讨显现运动时刻即便超越30分钟脂肪也只能焚烧6小时。\n",
      "label:  30分钟\n",
      "------\n",
      "Input:  问题:炼狱魔女多少钱 上下文:关于lol炼狱魔女蔚皮肤怎么样,多少钱,值不值得购买 皮肤名称:炼狱魔女 蔚 上架时间:2015年10月28日10:00 销售价格:6900点券 从上面炼狱魔女蔚的皮肤特效视频之中看来,觉得这次的炼狱魔女蔚的皮肤特效还是蛮不错的,已经值69元了,各位喜欢蔚的玩家快快去购买吧。\n",
      "label:  88元\n",
      "------\n",
      "Input:  问题:宁夏省有多少个市 上下文:宁夏回族自治区行政区域划分为5个地级市9个市辖区、2个县级市、11个县永宁县、贺兰县、平罗县、同心县、盐池县、西吉县、隆德县、泾源县、彭阳县、中宁县、海原县另外还辖1个开发区|1.银川市兴庆区 金凤区 西夏区 灵武市东塔镇 永宁县杨和街道 贺兰县习岗街道2.石嘴山市大武口区 惠农区 平罗县城关镇3.吴忠市利通区 青铜峡市小坝镇 同心县豫海镇 盐池县花马池镇 红寺堡开发区红寺堡镇 太阳山开发区太阳山镇4.固原市原州区 西吉县吉强镇 隆德县城关镇 泾源县香水镇 彭阳县白阳镇5.中卫市沙坡头区 中宁县宁安镇 海原县海城镇\n",
      "label:  5个\n",
      "------\n",
      "Input:  问题:种一颗牙需要多少钱 上下文:要想知道种植牙的价格我们应该先了解种植牙的整体结构才能知道整个种植牙的价格。种植牙主要由种植体、牙冠、基台等组成,也就是说种植牙的费用主要和这几个组成部分相关,其因素主要受以下几个方面的影响: 1、种植体:种植体是种植牙价格主要的组成部分,种植体是由钛元素组成,钛元素与人的结合度非常好,而且对人无任何副作用,并且种植体都是进口的价格自然贵一些。 2、牙冠:牙冠的材料主要有烤瓷牙、全瓷牙等,材料不相同则价格也不相同,全瓷牙价格要比烤瓷牙贵一些。 种植牙的费用因不同的产品和技术而有差异,相比于传统的假牙修复价格相对较高。种植牙都是以颗来计算价格的,一颗与多颗的价格自然不同,这还是需要看个人的牙齿具体情况来定。一般一颗种植牙的价格在几千到上万不等,具体的还要看其个人的选择。\n",
      "label:  几千到上万\n",
      "------\n",
      "Input:  问题:橱柜宽度 上下文:平面操作区域进深即宽度以40至60厘米为宜要充分考虑洗菜盆的宽度。以标准洗菜盆来算应选择550600MM的宽度为好。另在高度方面根据我国人体高度测算掌握以下尺寸为宜操作台高度在89至92厘米为宜平面操作区域进深以40至60厘米为宜抽油烟机与灶台的距离掌握在60至80厘米为宜操作台上方的吊柜要能使主人操作时不碰头为宜它距地面最小距离不应小于145厘米进深尺寸为25至35厘米吊柜与操作台之间的距离应在55厘米以上。\n",
      "label:  30至60厘米\n",
      "------\n",
      "Input:  问题:林芝旅游最佳时间 上下文:冬天很不错。可以给你推荐一个攻略。出自51星旅网(中国旅行社总社重庆)。 上午游览世界上海拔最高的古代宫堡式建筑群【布达拉宫】(门票200元,游览时间不低于1小时),历世达赖喇嘛驻地和旧西藏政教权力的中心。午餐后游览供奉有随行文成公主入藏的释迦牟尼十二岁等身像,藏传佛教信徒心中的圣地【大昭寺】(门票85元,游览时间不低于45分钟)。后参观国家2A级景区【甘露藏药工业园】(游览时间不低于60分钟)。游览结束后去【八廓街】(游览时间不低于30分钟)手工艺品市场,感受西藏的人文风土,浏览民族特色和异域风情的手工制品。在附近土特产超市自行选购当地特产60分钟左右。 由于布达拉宫(文物保护)门票每日限量销售,参观时间可能提前或延后,因此参观布达拉宫时间可能会影响到整体线路的先后调整,在保证服务标准及不减少旅游景点\n",
      "label:  年\n",
      "------\n",
      "Input:  问题:几月份去斯里兰卡最好 上下文:斯里兰卡地处热带属热带海洋性气候但受海风影响并不酷热。全年无四季之分只有雨季和旱季的差别雨季为每年5月至8月和11月至次年2月。相对来说9月至10月的旱季适宜旅游。在冬季的12月到次年3月许多欧洲人到斯里兰卡过冬尤其是圣诞节到元旦之间是旅游旺季这时候酒店定房比较紧张。另外的旺季是7月-8月是当地的佛牙节。在雨季淡季旅行的好处是机票和房价都比较便宜而且这时候也不一定一直下雨。小吃方面斯里兰卡的红茶大米椰肉都别具一番风味。\n",
      "label:  7月-8月\n",
      "------\n",
      "Input:  问题:文静老公是谁 上下文:值得网友好奇的莫过于yy文静是否已经结婚,老公是谁了,据悉赵文静已经结婚,而且还是同门。|赵文静的老公正是赵本山的42号弟子刘传龙;两人于2011年6月3日在老家辽阳市某豪华酒店举办了隆重婚礼;婚礼当天来了不少刘传龙的师兄弟们,让现场气氛十分活跃。|在婚礼现场两人提及爱情心路和父母及恩师赵本山情感时,感动了现场所有的嘉宾,两人在台上更是一度流下幸福的泪水。虽然当时赵本山因为忙于拍戏没能赶到现场,但把自己的所有祝福和期望以视频的方式传递到了现场,让两人在现场多次感动的哽咽说不出话。\n",
      "label:  赵文安\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "valid_data = next(iter(valid_dataloader))\n",
    "batch_data = valid_data.to(device)\n",
    "outputs = model.generate(\n",
    "    batch_data[\"input_ids\"],\n",
    "    attention_mask=batch_data[\"attention_mask\"],\n",
    "    max_new_tokens=MAX_TARGET_LENGTH,\n",
    "    num_beams=4\n",
    "    )\n",
    "decoded_outputs = tokenizer.batch_decode(\n",
    "    outputs,\n",
    "    skip_special_tokens=True\n",
    "    )\n",
    "for input, label in zip(\n",
    "    tokenizer.batch_decode( batch_data[\"input_ids\"], skip_special_tokens=True), \n",
    "    decoded_outputs):\n",
    "    print(\"Input: \",input )\n",
    "    print(\"label: \", label)\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e016c699",
   "metadata": {},
   "source": [
    "## More epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cc36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "epoch_num = 5 # prev section epochs\n",
    "additional_epochs = 5\n",
    "best_model_name = \"best_t5.pt\"\n",
    "\n",
    "foldername =  '31-01-26-15_14_ckpt'\n",
    "checkpoint_path = Path(f\"./checkpoint/{foldername}\")\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "prev_file_path = checkpoint_path / best_model_name\n",
    "\n",
    "foldername =  '31-01-26-15_14_more_ckpt'\n",
    "checkpoint_path = Path(f\"./checkpoint/{foldername}\")\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "file_path = checkpoint_path / best_model_name\n",
    "\n",
    "recent_checkpoints = []\n",
    "use_wandb = True\n",
    "\n",
    "model.load_state_dict(torch.load(prev_file_path, weights_only=True))\n",
    "\n",
    "num_training_steps = additional_epochs * len(train_dataloader)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "global_step = global_step\n",
    "best_bleu = 0\n",
    "for epoch in range(additional_epochs):\n",
    "    avg_loss, global_step = train_loop(train_dataloader, model, optimizer, scheduler, epoch + epoch_num, global_step, use_wandb=use_wandb)\n",
    "    valid_bleu = valid_loop(valid_dataloader, model, tokenizer, epoch + epoch_num, global_step, use_wandb=use_wandb)\n",
    "    bleu_avg = valid_bleu['avg']\n",
    "    save_checkpoint(model, epoch + epoch_num, checkpoint_path, recent_checkpoints)\n",
    "    if bleu_avg > best_bleu:\n",
    "        best_bleu = bleu_avg \n",
    "        print(\"Saving new best weights ...\")\n",
    "        torch.save(model.state_dict() , file_path)\n",
    "        print(\"Finish saving.\")\n",
    "    \n",
    "\n",
    "print(\"Finish training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
