{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e96d5b",
   "metadata": {},
   "source": [
    "# Call library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e51419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import torch\n",
    "import os\n",
    "import evaluate \n",
    "import wandb\n",
    "import numpy as np\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, get_scheduler\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from utils import save_checkpoint, read_json, get_data_stats, collote_train_fn, collote_valid_fn, MAX_TARGET_LENGTH\n",
    "from dataset import MengziT5Dataset\n",
    "from pathlib import Path\n",
    "from datetime import datetime \n",
    "from tqdm import tqdm \n",
    "from dotenv import load_dotenv \n",
    "load_dotenv()\n",
    "\n",
    "checkpoint = \"Langboat/mengzi-t5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b4671",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4089b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_qa_dataset(data, output_file_path):\n",
    "    \"\"\"\n",
    "    Merges JSON entries with the same Context and Question into a single entry\n",
    "    with a list of answers. Re-indexes IDs sequentially.\n",
    "    \"\"\"\n",
    "    # Grouping Logic\n",
    "    # We use a dictionary to group items.\n",
    "    # Key: Tuple of (context, question) -> This ensures unique QA pairs\n",
    "    # Value: List of answers\n",
    "    grouped_data = {}\n",
    "\n",
    "    print(f\"Processing {len(data)} items...\")\n",
    "\n",
    "    for item in data:\n",
    "        context = item.get('context', '').strip()\n",
    "        question = item.get('question', '').strip()\n",
    "        answer = item.get('answer', '')\n",
    "        \n",
    "        # Create a unique key for this specific question context\n",
    "        key = (context, question)\n",
    "\n",
    "        if key not in grouped_data:\n",
    "            grouped_data[key] = []\n",
    "\n",
    "        # Handle cases where input answer might already be a list or a string\n",
    "        if isinstance(answer, list):\n",
    "            grouped_data[key].extend(answer)\n",
    "        else:\n",
    "            grouped_data[key].append(answer)\n",
    "\n",
    "    # 3. Reconstruct the List with new IDs\n",
    "    new_json_data = []\n",
    "    new_id_counter = 0\n",
    "\n",
    "    for (context, question), answers in grouped_data.items():\n",
    "        # Remove duplicate answers if you want unique references only\n",
    "        answers = list(set(answers)) \n",
    "        \n",
    "        entry = {\n",
    "            \"id\": new_id_counter,\n",
    "            \"context\": context,\n",
    "            \"question\": question,\n",
    "            \"answer\": answers  # This is now a list [\"Ans1\", \"Ans2\"]\n",
    "        }\n",
    "        new_json_data.append(entry)\n",
    "        new_id_counter += 1\n",
    "\n",
    "    # Save to new file\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        for obj in tqdm(new_json_data, desc=\"Writing to JSON file\"):\n",
    "            # ensure_ascii=False is crucial for Chinese characters to be readable\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"Success! Merged data saved to {output_file_path}\")\n",
    "    print(f\"Original count: {len(data)} -> New count: {len(new_json_data)}\")\n",
    "\n",
    "    return new_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679b5ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSON file: 984it [00:00, 144580.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 984 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing to JSON file: 100%|██████████| 700/700 [00:00<00:00, 93206.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Merged data saved to data/formatted_dev.json\n",
      "Original count: 984 -> New count: 700\n",
      "First valid data:  {'id': 0, 'context': '年基准利率4.35%。 从实际看,贷款的基本条件是: 一是中国大陆居民,年龄在60岁以下; 二是有稳定的住址和工作或经营地点; 三是有稳定的收入来源; 四是无不良信用记录,贷款用途不能作为炒股,赌博等行为; 五是具有完全民事行为能力。', 'question': '2017年银行贷款基准利率', 'answer': ['年基准利率4.35%', '4.35%']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading JSON file: 14520it [00:00, 158519.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First train data:  {'context': '第35集雪见缓缓张开眼睛，景天又惊又喜之际，长卿和紫萱的仙船驶至，见众人无恙，也十分高兴。众人登船，用尽合力把自身的真气和水分输给她。雪见终于醒过来了，但却一脸木然，全无反应。众人向常胤求助，却发现人世界竟没有雪见的身世纪录。长卿询问清微的身世，清微语带双关说一切上了天界便有答案。长卿驾驶仙船，众人决定立马动身，往天界而去。众人来到一荒山，长卿指出，魔界和天界相连。由魔界进入通过神魔之井，便可登天。众人至魔界入口，仿若一黑色的蝙蝠洞，但始终无法进入。后来花楹发现只要有翅膀便能飞入。于是景天等人打下许多乌鸦，模仿重楼的翅膀，制作数对翅膀状巨物。刚佩戴在身，便被吸入洞口。众人摔落在地，抬头发现魔界守卫。景天和众魔套交情，自称和魔尊重楼相熟，众魔不理，打了起来。', 'answer': '第35集', 'question': '仙剑奇侠传3第几集上天界', 'id': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = \"data/train.json\"\n",
    "DATA_DEV_PATH = \"data/dev.json\"\n",
    "\n",
    "DATA_FDEV_PATH = \"data/formatted_dev.json\"\n",
    "DATA_DEV_PATH = \"data/dev.json\"\n",
    "\n",
    "valid_data = read_json(DATA_DEV_PATH)\n",
    "merged_valid_data = merge_qa_dataset(valid_data, DATA_FDEV_PATH)\n",
    "# merged_valid_data = read_json(DATA_FDEV_PATH)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint) \n",
    "\n",
    "print(\"First valid data: \", merged_valid_data[0])\n",
    "train_data = read_json(DATA_TRAIN_PATH)\n",
    "print(\"First train data: \", train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8fec6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_num': 984,\n",
       " 'context_num': 984,\n",
       " 'answer_num': 984,\n",
       " 'question_mean_length': 6.5426829268292686,\n",
       " 'context_mean_length': 192.15243902439025,\n",
       " 'answer_mean_length': 4.774390243902439,\n",
       " 'question_max_length': 18,\n",
       " 'context_max_length': 728,\n",
       " 'answer_max_length': 26}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_stats(valid_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09942fec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question_num': 14520,\n",
       " 'context_num': 14520,\n",
       " 'answer_num': 14520,\n",
       " 'question_mean_length': 6.488154269972452,\n",
       " 'context_mean_length': 182.3798209366391,\n",
       " 'answer_mean_length': 4.257782369146006,\n",
       " 'question_max_length': 28,\n",
       " 'context_max_length': 1180,\n",
       " 'answer_max_length': 95}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_stats(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9388d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data filtered away: 19\n",
      "Total data filtered away: 538\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = MengziT5Dataset(merged_valid_data, tokenizer)\n",
    "train_dataset = MengziT5Dataset(train_data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa0fd0",
   "metadata": {},
   "source": [
    "# Retrieve Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9497a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 8\n",
    "valid_batch_size = 8\n",
    "#test_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc1aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 282/282 [00:00<00:00, 567.66it/s, Materializing param=shared.weight]                                                       \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input_ids:  tensor([[  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0]])\n",
      "train attention_mask:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "train decoder_input_ids tensor([[    0,  7973,  5946,  ...,     0,     0,     0],\n",
      "        [    0,  7973,  2056,  ...,     0,     0,     0],\n",
      "        [    0, 12598,    50,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0,     7,   170,  ...,     0,     0,     0],\n",
      "        [    0,  8921, 21976,  ...,     0,     0,     0],\n",
      "        [    0,     7,  6390,  ...,     0,     0,     0]])\n",
      "train labels tensor([[ 7973,  5946,   212,  ...,  -100,  -100,  -100],\n",
      "        [ 7973,  2056,   101,  ...,  -100,  -100,  -100],\n",
      "        [12598,    50,    99,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [    7,   170,   291,  ...,  -100,  -100,  -100],\n",
      "        [ 8921, 21976,     1,  ...,  -100,  -100,  -100],\n",
      "        [    7,  6390,   278,  ...,  -100,  -100,  -100]])\n",
      "----------\n",
      "valid input_ids:  tensor([[  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0],\n",
      "        [  7, 143,  13,  ...,   0,   0,   0]])\n",
      "valid attention_mask:  tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])\n",
      "valid decoder_input_ids:  tensor([[    0,     7,   419,  ...,     0,     0,     0],\n",
      "        [    0, 12598,  9787,  ...,     0,     0,     0],\n",
      "        [    0,  8811,    92,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    0,     7,  3485,  ...,     0,     0,     0],\n",
      "        [    0,     7,   280,  ...,     0,     0,     0],\n",
      "        [    0, 11384,   707,  ...,     0,     0,     0]])\n",
      "valid labels: tensor([[    7,   419,   725,  ...,  -100,  -100,  -100],\n",
      "        [12598,  9787,   101,  ...,  -100,  -100,  -100],\n",
      "        [ 8811,    92,     1,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [    7,  3485,     1,  ...,  -100,  -100,  -100],\n",
      "        [    7,   280, 15245,  ...,  -100,  -100,  -100],\n",
      "        [11384,   707,   296,  ...,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "model = model.to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=train_batch_size, collate_fn=lambda x: collote_train_fn(x, model, tokenizer))\n",
    "train_data = next(iter(train_dataloader))\n",
    "print(\"train input_ids: \", train_data['input_ids'])\n",
    "print(\"train attention_mask: \", train_data['attention_mask'])\n",
    "print(\"train decoder_input_ids\", train_data['decoder_input_ids'])\n",
    "print(\"train labels\", train_data['labels'])\n",
    "print(\"----------\")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "valid_dataset, _ = random_split(valid_dataset, [0.5, 0.5], generator=generator)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_dataset, shuffle=False, batch_size=valid_batch_size, collate_fn=lambda x: collote_valid_fn(x, model, tokenizer))\n",
    "valid_data = next(iter(valid_dataloader))\n",
    "print(\"valid input_ids: \", valid_data['input_ids'])\n",
    "print(\"valid attention_mask: \", valid_data['attention_mask'])\n",
    "print(\"valid decoder_input_ids: \", valid_data['decoder_input_ids'])\n",
    "print(\"valid labels:\", valid_data['labels'])\n",
    "\n",
    "# test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=valid_batch_size, collate_fn=lambda x: collote_fn(x, model, tokenizer))\n",
    "# test_data = next(iter(test_dataloader))\n",
    "# print(\"test input_ids: \", test_data['input_ids'])\n",
    "# print(\"test attention_mask: \", test_data['attention_mask'])\n",
    "# print(\"test decoder_input_ids: \", test_data['decoder_input_ids'])\n",
    "# print(\"test labels:\", test_data['labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917848ee",
   "metadata": {},
   "source": [
    "# Train Model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7350c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, optimizer, scheduler, epoch, global_step, use_wandb=False):\n",
    "    model.train()\n",
    "    # Reset loss counter at the start of the epoch\n",
    "    epoch_loss_sum = 0.0 \n",
    "    current_avg_loss = 0.0\n",
    "    #cumulative_batch = len(dataloader) * (epoch - 1)\n",
    "    \n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        for batch_idx, batch_data in enumerate(dataloader, start=1):\n",
    "            batch_data = batch_data.to(device)\n",
    "            results = model(**batch_data)\n",
    "            loss = results.loss\n",
    "\n",
    "            # backward popagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    {\"train_loss\": loss.item()},\n",
    "                    step=global_step\n",
    "                )\n",
    "\n",
    "            epoch_loss_sum += loss.item()\n",
    "            current_avg_loss = epoch_loss_sum / batch_idx\n",
    "\n",
    "            pbar.set_description(f\"Epoch {epoch} | Avg Loss: {current_avg_loss:.4f}\")\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "    return current_avg_loss, global_step \n",
    "\n",
    "def valid_loop(dataloader, model, tokenizer, epoch, global_step, use_wandb=False):\n",
    "    model.eval()\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    loss = []\n",
    "    val_loss_sum = 0.0\n",
    "\n",
    "    #cumulative_batch = (epoch-1) * len(dataloader)\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with tqdm(total=len(dataloader)) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch_data in enumerate(dataloader, start=1):\n",
    "                raw_references = batch_data.pop(\"answer\", None)\n",
    "                if raw_references is None:\n",
    "                    print(\"No raw reference is found. Now create based on labels.\")\n",
    "                    temp_labels = torch.where(batch_data[\"labels\"] != -100, batch_data[\"labels\"], tokenizer.pad_token_id)\n",
    "                    raw_references = [[ref] for ref in tokenizer.batch_decode(temp_labels, skip_special_tokens=True)]\n",
    "\n",
    "\n",
    "                batch_data = batch_data.to(device)\n",
    "                results = model(**batch_data)\n",
    "                loss = results.loss\n",
    "                val_loss_sum += loss.item() # Accumulate loss\n",
    "\n",
    "                outputs = model.generate(\n",
    "                    batch_data[\"input_ids\"],\n",
    "                    attention_mask=batch_data[\"attention_mask\"],\n",
    "                    max_new_tokens=MAX_TARGET_LENGTH,\n",
    "                    num_beams=4\n",
    "                    )\n",
    "                decoded_outputs = tokenizer.batch_decode(\n",
    "                    outputs,\n",
    "                    skip_special_tokens=True\n",
    "                    )\n",
    "                # labels = batch_data['labels']\n",
    "                # labels = torch.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "                # decoded_labels = tokenizer.batch_decode(\n",
    "                #     labels,\n",
    "                #     skip_special_tokens=True\n",
    "                # )\n",
    "\n",
    "                batch_preds = []\n",
    "                for pred in decoded_outputs:\n",
    "                    if len(pred) == 0:\n",
    "                        pred = \" \" # Prevent divided by zero during calculation of BLEU\n",
    "                    batch_preds.append(pred)\n",
    "                \n",
    "                batch_labels = []\n",
    "                for ref_list in raw_references: # ref_list: [ans1, ans2, ...]\n",
    "                    processed_ref_list = []\n",
    "                    for ref in ref_list:\n",
    "                        cleaned_ref = ref.strip()\n",
    "                        processed_ref_list.append(' '.join(cleaned_ref.strip()))\n",
    "                    batch_labels.append(processed_ref_list)\n",
    "\n",
    "                # batch_preds = [' '.join(pred.strip()) for pred in decoded_outputs]\n",
    "                # batch_labels = [' '.join(label.strip()) for label in decoded_labels]\n",
    "                print(f\"First data: decoded output: {decoded_outputs[0]}, ref: {raw_references[0]}\")\n",
    "                all_preds.extend(batch_preds)\n",
    "                all_labels.extend(batch_labels)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "            bleu_result = bleu.compute(predictions=all_preds, references=all_labels)\n",
    "            result = {f\"bleu-{i}\" : value for i, value in enumerate(bleu_result[\"precisions\"], start=1)}\n",
    "            result['avg'] = bleu_result['bleu']\n",
    "            avg_val_loss = val_loss_sum / len(dataloader)\n",
    "            log_dict = {\n",
    "                \"val_loss\": avg_val_loss,\n",
    "                \"BLEU_avg\": bleu_result['bleu'], # 'bleu' is the avg in huggingface evaluate\n",
    "                \"BLEU_1\": bleu_result['precisions'][0],\n",
    "                \"BLEU_2\": bleu_result['precisions'][1],\n",
    "                \"BLEU_3\": bleu_result['precisions'][2],\n",
    "                \"BLEU_4\": bleu_result['precisions'][3],\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            if use_wandb:\n",
    "                wandb.log(\n",
    "                    log_dict,\n",
    "                    step=global_step\n",
    "                )\n",
    "            print(f\"Test result: BLEU_avg={result['avg']}, BLEU1={result['bleu-1']}, BLEU2={result['bleu-2']}, BLEU3={result['bleu-3']}, BLEU4={result['bleu-4']}\")\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5366ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from WANDB_API_KEY.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlamyeungkong0108\u001b[0m (\u001b[33mlamyeungkong0108-the-hong-kong-university-of-science-and\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/mengzi-t5-QuestionAnswering-/mengzi-t5-base/wandb/run-20260202_140439-ca1gz3ie</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa/runs/ca1gz3ie' target=\"_blank\">02-02-26-14_04</a></strong> to <a href='https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa' target=\"_blank\">https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa/runs/ca1gz3ie' target=\"_blank\">https://wandb.ai/lamyeungkong0108-the-hong-kong-university-of-science-and/mengzi-t5-qa/runs/ca1gz3ie</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Avg Loss: 8.0900: 100%|██████████| 1748/1748 [05:53<00:00,  4.95it/s]\n",
      "100%|██████████| 43/43 [01:16<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: BLEU_avg=0.0, BLEU1=0.023484434735117424, BLEU2=0.003209242618741977, BLEU3=0.0, BLEU4=0.0\n",
      "Saving checkpoint to checkpoint/02-02-26-14_04_ckpt/ckpt-epoch0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Avg Loss: 6.8229: 100%|██████████| 1748/1748 [05:52<00:00,  4.96it/s]\n",
      "100%|██████████| 43/43 [01:15<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test result: BLEU_avg=0.0, BLEU1=0.01790710688304421, BLEU2=0.0031308703819661866, BLEU3=0.0006548788474132286, BLEU4=0.0\n",
      "Saving checkpoint to checkpoint/02-02-26-14_04_ckpt/ckpt-epoch1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Avg Loss: 6.0911:  83%|████████▎ | 1451/1748 [04:53<01:00,  4.95it/s]\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m best_bleu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_num):\n\u001b[0;32m---> 36\u001b[0m     avg_loss, global_step \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_wandb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     valid_bleu \u001b[38;5;241m=\u001b[39m valid_loop(valid_dataloader, model, tokenizer, epoch, global_step, use_wandb\u001b[38;5;241m=\u001b[39muse_wandb)\n\u001b[1;32m     38\u001b[0m     bleu_avg \u001b[38;5;241m=\u001b[39m valid_bleu[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, optimizer, scheduler, epoch, global_step, use_wandb)\u001b[0m\n\u001b[1;32m     20\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_wandb:\n\u001b[1;32m     22\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog(\n\u001b[0;32m---> 23\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m},\n\u001b[1;32m     24\u001b[0m         step\u001b[38;5;241m=\u001b[39mglobal_step\n\u001b[1;32m     25\u001b[0m     )\n\u001b[1;32m     27\u001b[0m epoch_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     28\u001b[0m current_avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss_sum \u001b[38;5;241m/\u001b[39m batch_idx\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x7f986c3f96c0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f9673112590, execution_count=10 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 7f9673111960, raw_cell=\"learning_rate = 2e-5\n",
      "epoch_num = 5\n",
      "best_model_name..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22737368202d7020333436363320726f6f7440636f6e6e6563742e637161312e7365657461636c6f75642e636f6d227d/home/mengzi-t5-QuestionAnswering-/mengzi-t5-base/train.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n",
      "socket.send() raised exception.\n"
     ]
    },
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:607\u001b[0m, in \u001b[0;36m_WandbInit._post_run_cell_hook\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:811\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 811\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[0m, in \u001b[0;36mInterfaceSock._publish\u001b[0;34m(self, record, nowait)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncer\u001b[38;5;241m.\u001b[39mrun_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mpublish(request))\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_asyncer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[0m, in \u001b[0;36mAsyncioManager.run\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    133\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule(fn, daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[0m, in \u001b[0;36mAsyncioManager._wrap\u001b[0;34m(self, fn, daemon, name)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task \u001b[38;5;241m:=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mcurrent_task()):\n\u001b[1;32m    217\u001b[0m         task\u001b[38;5;241m.\u001b[39mset_name(name)\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[0m, in \u001b[0;36mServiceClient.publish\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_server_request(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[0m, in \u001b[0;36mServiceClient._send_server_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     61\u001b[0m data \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39mdrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/asyncio/streams.py:359\u001b[0m, in \u001b[0;36mStreamWriter.drain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mexception()\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mis_closing():\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/lib/asyncio_manager.py:181\u001b[0m, in \u001b[0;36mAsyncioManager.run_soon.<locals>.fn_wrap_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfn_wrap_exceptions\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 181\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m         _logger\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUncaught exception in run_soon callback.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[0m, in \u001b[0;36mServiceClient.publish\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb\u001b[38;5;241m.\u001b[39mServerRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_server_request(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[0m, in \u001b[0;36mServiceClient._send_server_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     61\u001b[0m data \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mSerializeToString()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writer\u001b[38;5;241m.\u001b[39mdrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/asyncio/streams.py:359\u001b[0m, in \u001b[0;36mStreamWriter.drain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mexception()\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39mis_closing():\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/my_ai_env/lib/python3.10/asyncio/selector_events.py:924\u001b[0m, in \u001b[0;36m_SelectorSocketTransport.write\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer:\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;66;03m# Optimization: try to send now.\u001b[39;00m\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m         n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mBlockingIOError\u001b[39;00m, \u001b[38;5;167;01mInterruptedError\u001b[39;00m):\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "learning_rate = 2e-5\n",
    "epoch_num = 5\n",
    "best_model_name = \"best_t5.pt\"\n",
    "current_t = datetime.now().strftime('%d-%m-%y-%H_%M')\n",
    "foldername =  current_t + '_ckpt'\n",
    "checkpoint_path = Path(f\"./checkpoint/{foldername}\")\n",
    "checkpoint_path.mkdir(parents=True, exist_ok=True)\n",
    "file_path = checkpoint_path / best_model_name\n",
    "recent_checkpoints = []\n",
    "use_wandb = True\n",
    "\n",
    "if use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"mengzi-t5-qa\",   # The name of project on the website\n",
    "        name=f\"{current_t}\",  # Name of this specific training run\n",
    "        config={        \n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"batch_size\": train_batch_size,\n",
    "            \"epochs\": epoch_num,\n",
    "            \"model\": \"mengzi-t5-base\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "num_training_steps = epoch_num * len(train_dataloader)\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "global_step = 0\n",
    "best_bleu = 0\n",
    "for epoch in range(epoch_num):\n",
    "    avg_loss, global_step = train_loop(train_dataloader, model, optimizer, scheduler, epoch, global_step, use_wandb=use_wandb)\n",
    "    valid_bleu = valid_loop(valid_dataloader, model, tokenizer, epoch, global_step, use_wandb=use_wandb)\n",
    "    bleu_avg = valid_bleu['avg']\n",
    "    save_checkpoint(model, epoch, checkpoint_path, recent_checkpoints)\n",
    "    if bleu_avg > best_bleu:\n",
    "        best_bleu = bleu_avg \n",
    "        print(\"Saving new best weights ...\")\n",
    "        torch.save(model.static_dict() , file_path)\n",
    "        print(\"Finish saving.\")\n",
    "    \n",
    "\n",
    "print(\"Finish training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb3a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['问题:防晒伞什么牌子好 上下文:喜途汽车安全伞就不错啊伞用起来不错做工挺精致的布料也比较厚实外形特别的漂亮。|防晒品的作用是阻止阳光中紫外线的照射牌子有相宜本草兰芝欧莱雅等。防晒的方法是多吃蔬菜和水果戴帽子、打遮阳伞、穿长袖外衣。', '问题:苹果6换苹果7多少钱呀 上下文:如果你要用苹果6补差换购全新正品苹果7可以到下面苹果实体店补差换购哈,他们哪里差不只需要补4000左右就可以换全新正品苹果7手机了哈。 苹果专卖店(赛格店) 地址:太升南路222号赛格广场4楼1032号', '问题:屠呦呦获诺贝尔奖奖金是多少 上下文:约46万美元。今年生理学或医学奖奖金共800万瑞典克朗约合92万美元屠呦呦将获得奖金的一半另外两名科学家将共享奖金的另一半。诺贝尔奖的奖金总是以瑞典的货币瑞典克朗颁发在同一年里各项奖金的数额是相同的不同的年份奖金数额有所变动其幅度主要取决于市场行情。每年的奖金金额视诺贝尔基金的投资收益而定1901年第一次颁奖的时候每单项的奖金为15万瑞典克朗当时相当于瑞典一个教授工作20年的薪金。1980年诺贝尔奖的单项奖金达到100万瑞典克朗1991年为600万瑞典克朗1992年为650万瑞典克朗1993年为670万瑞典克朗2000年单项奖金达到了900万瑞典克朗当时约折合100万美元。从2001年到2011年单项奖金均为1000万瑞典克朗在2011年折合约145万美元。', '问题:盾构机一台多少钱 上下文:一般直径的地铁盾构(也就是直径6米的)大约4000万左右(国内盾构生产厂商),国外的(如德国、美国)会稍微贵一些(日本可能会便宜一些)。大直径的盾构机会更贵,具体价格视盾构直径而定,当然土压平衡盾构和泥水平衡盾构的价钱也不一样,简单来说泥水平衡盾构会比相同直径的土压平衡盾构贵一些。', '问题:宁夏旅游几月份去好 上下文:宁夏的温差很大春季沙尘冬季寒冷干燥。所以建议你夏天去宁夏吧。最好是五一至十一之间。1、五月和九月白天的温度可以穿裙子但早晚凉。不适合下水游泳。冬泳选手例外。六月时可以摘樱桃等七月八月瓜果成熟尤其是西瓜包沙包甜哈密瓜、香瓜等都甜的很九月的葡萄绝对甜而且皮薄宁夏的葡萄可是酿酒的原材料啊地理条件和气候都类似法国所以葡萄质量绝对可靠。还有丰收的枸杞是特产是良药可入菜和粥。沙枣子呵呵和普通的大枣绝对不同包你印象深刻。2、代表性的玩沙湖、镇北堡西部影视城、西夏王陵、贺兰山苏峪口或滚钟口、108塔、沙坡头、六盘山、老龙潭。由北向南玩。如果你不是回民那清真寺你大概只有在外面看看的份了。这些地方囊括了沙漠、黄河、山岭和平原地带可以在有沙子的地方玩卡丁车、滑沙黄河边上一定要坐羊皮筏子影视城里找找大话西游那些个景点自己也站上去说一段刻骨铭心的表白吧。别忘了吃宁夏的羊肉不膻不腻早点可吃羊杂碎和羊肉泡馍午饭可吃羊肉搓面、羊肉臊子面、羊羔肉晚饭可吃涮羊肉、手抓肉夜宵可吃烤羊肉串、肉夹馍等。', '问题:收银台高度 上下文:1、收银台高度得尺寸以立姿作业得人体尺寸参数为参考,50百分位女性得肘高为960mm,通过科学研究发现最舒适的站姿工作高度时低于人的肘部高度约7.6cm,结合修正量计算得,收银台的高度在800mm为宜。 2、收银台宽度:受操作的最大范围为720mm左右,收银员与台面距离为50～200mm不等,因此,台面宽度不宜过大,但最小要容纳一个购物筐。', '问题:iphone7电池容量 上下文:在实际续航测试中iPhone 7相比上代6s延长了2小时而iPhone 7 Plus则比6s Plus提高了1小时。但发布会现场暂时未透露具体的容量大校据传iPhone7的电池容量为1960mAhiPhone7plus的电池容量为3100mAh。', '问题:微信续签港澳通行证要多久 上下文:港澳通行证续签一般需要7个工作日内就可以领取的若当地具有自助签注机的情况公民只需要带上港澳通行证原件卡式以及本人身份证原件前往办理当场就可以办理续签。|公安机关出入境管理部门受理往来港澳通行证申请后应当在10个工作日内予以签发单独受理签注申请后应当在7个工作日内予以签发。因所在地区交通不便或者因特殊情况不能按期签发的经省级公安机关出入境管理部门批准签发时间可以延长至20个工作日。|按照公安部有关规定在非常住户口所在地申请往来港澳通行证、签注的公安机关出入境管理部门应当在30日内予以签发。|内地居民因赴港澳地区治病、探望危重病人、奔丧等特殊情况急需申请往来港澳通行证、签注的公安机关出入境管理部门应当按照急事急办原则优先审批办理。|续签需要港澳通行证、数码照片、户口复印件、身份证复印件签注需要2周办好年满16周岁和成年人一样不需要监护人陪同签注。']\n",
      "['', '2000左右', '', '2000万', '月', '500mm', '1000', '                                                                                                                               ']\n"
     ]
    }
   ],
   "source": [
    "valid_data = next(iter(valid_dataloader))\n",
    "batch_data = valid_data.to(device)\n",
    "outputs = model.generate(\n",
    "    batch_data[\"input_ids\"],\n",
    "    attention_mask=batch_data[\"attention_mask\"],\n",
    "    max_new_tokens=MAX_TARGET_LENGTH,\n",
    "    num_beams=4\n",
    "    )\n",
    "decoded_outputs = tokenizer.batch_decode(\n",
    "    outputs,\n",
    "    skip_special_tokens=True\n",
    "    )\n",
    "print(tokenizer.batch_decode(\n",
    "    batch_data[\"input_ids\"],\n",
    "    skip_special_tokens=True\n",
    "    ))\n",
    "print(decoded_outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
